{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYdAR2EJLhQuxnsUgSg+LU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakariajaadi/data-science-portofolio/blob/main/Byte_Pair_Encoding_Algorithm_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Pair Encoding tokenizer from scratch ðŸ”¢"
      ],
      "metadata": {
        "id": "nKboqKaVGYbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition** :\n",
        "\n",
        "* Byte Pair Encoding is a subword tokenization method that is used by a lot of Transformer models such as GPT, GPT-2, RoBERTa, BART, and DeBERTa.\n",
        "* It uses a learned vocabulary to break down a text into subword tokens, which can include characters, parts of words, or whole words.\n",
        "\n",
        "**Why subwords ?**\n",
        "\n",
        "Using subwords avoids the need for a massive vocabulary that includes every possible word. for example a word \"quickest\" might not be present in the vocabulary, but can be represented with two subwords tokens `[\"quick\", \"est\"]`.\n",
        "\n",
        "**Algorithm**:\n",
        "\n",
        "1. **Initialization**: Begin with a vocabulary consisting of the unique characters found in your training corpus, along with the special end-of-word token.\n",
        "2. **Most frequent Pair Identification**: Identify the most frequently occurring adjacent pair of symbols within the words of the corpus.\n",
        "3. **In-Word Merging**: Merge the identified pair into a new single symbol. Crucially, this merging operation is performed directly within the words of your corpus, replacing all occurrences of the pair with the new merged symbol. Add this new merged symbol to your vocabulary.\n",
        "4. **Iteration**: Repeat steps 2 and 3 for a predetermined number of iterations or until no more pairs can be merged.\n",
        "\n",
        "**The key insight** âœ¨\n",
        "\n",
        "The key insight is that this greedy approach of repeatedly merging the most frequent pairs will naturally discover common subword units like prefixes, suffixes, and stems that occur frequently in the language. This is why BPE is considered a subword tokenization method.\n",
        "\n",
        "The beauty of BPE is that it's completely data-driven. It doesn't need any linguistic knowledge about the language; it simply discovers patterns based on frequency statistics."
      ],
      "metadata": {
        "id": "eQQTsgiB3boF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- BPE implementation from scratch\n",
        "BPE requires a special token to mark the end of words. In this implementation, I am using `</w>`."
      ],
      "metadata": {
        "id": "PwGt_I8Ugfxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "class BPE:\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initializes the BPE model with a target vocabulary size.\"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.merges = {}  # Maps pairs to their rank for efficient lookup\n",
        "        self.vocab = {}   # Final vocabulary after training\n",
        "        self.token_to_id = {}  # Maps tokens to unique IDs\n",
        "        self.id_to_token = {}  # Maps IDs back to tokens\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        \"\"\"Preprocesses the text by lowercasing and splitting into words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            list: List of words\n",
        "        \"\"\"\n",
        "        # Split on whitespace and filter out empty strings\n",
        "        return [word for word in re.findall(r'\\S+', text.lower())]\n",
        "\n",
        "    def _get_initial_vocab(self, text):\n",
        "        \"\"\"Generates initial vocabulary from text, with each word split into characters.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping character-split words to frequencies\n",
        "        \"\"\"\n",
        "        vocab = defaultdict(int)\n",
        "        words = self._preprocess(text)\n",
        "\n",
        "        for word in words:\n",
        "            # Split word into characters and add end-of-word token\n",
        "            char_word = ' '.join(list(word)) + ' </w>'\n",
        "            vocab[char_word] += 1\n",
        "\n",
        "        return dict(vocab)\n",
        "\n",
        "    def _get_pair_stats(self, vocab):\n",
        "        \"\"\"Counts frequencies of adjacent pairs in the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            vocab (dict): Current vocabulary\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping character pairs to their frequencies\n",
        "        \"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            # Count pairs in each word\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def _merge_pair(self, pair, vocab):\n",
        "        \"\"\"Merges the specified pair in the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            pair (tuple): Pair of tokens to merge\n",
        "            vocab (dict): Current vocabulary\n",
        "\n",
        "        Returns:\n",
        "            dict: Updated vocabulary with merged pairs\n",
        "        \"\"\"\n",
        "        pair_str = ' '.join(pair)\n",
        "        merged = ''.join(pair)\n",
        "        new_vocab = {}\n",
        "\n",
        "        pattern = re.compile(r'(?<!\\S)' + re.escape(pair_str) + r'(?!\\S)')\n",
        "\n",
        "        for word, freq in vocab.items():\n",
        "            # Use regex for more accurate replacements of the specific pair\n",
        "            new_word = pattern.sub(merged, word)\n",
        "            new_vocab[new_word] = freq\n",
        "\n",
        "        return new_vocab\n",
        "\n",
        "    def train(self, text):\n",
        "        \"\"\"Trains the BPE model on the provided text.\n",
        "\n",
        "        Args:\n",
        "            text (str): Training text\n",
        "\n",
        "        Returns:\n",
        "            self: Trained BPE model\n",
        "        \"\"\"\n",
        "        vocab = self._get_initial_vocab(text)\n",
        "\n",
        "        print(\"#----------Training Started-----------#\")\n",
        "\n",
        "        # Perform merges up to vocab_size or until no more pairs can be merged\n",
        "        for i in range(self.vocab_size):\n",
        "            pair_stats = self._get_pair_stats(vocab)\n",
        "            if not pair_stats:\n",
        "                break\n",
        "\n",
        "            # Find the most frequent pair\n",
        "            best_pair = max(pair_stats, key=pair_stats.get)\n",
        "            best_freq = pair_stats[best_pair]\n",
        "\n",
        "            # Print progress info\n",
        "            print(f\"Iteration {i}: Merging {best_pair} with frequency {best_freq}\")\n",
        "\n",
        "            # Store merge with its rank (lower rank = higher priority)\n",
        "            self.merges[best_pair] = i\n",
        "\n",
        "            # Update vocabulary with the merged pair\n",
        "            vocab = self._merge_pair(best_pair, vocab)\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # Build token mappings after training\n",
        "        self._build_token_mappings()\n",
        "\n",
        "        print(f\"#----------Training Finished with {len(self.merges)} merges-----------#\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _build_token_mappings(self):\n",
        "        \"\"\"Builds token-to-ID and ID-to-token mappings from the trained vocabulary.\"\"\"\n",
        "        # Collect all unique tokens from the vocabulary\n",
        "        tokens = set()\n",
        "        for word in self.vocab:\n",
        "            tokens.update(word.split())\n",
        "\n",
        "        # Add special tokens if needed\n",
        "        tokens = sorted(tokens)  # Sort for deterministic IDs\n",
        "\n",
        "        # Map tokens to IDs and vice versa\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "        self.id_to_token = {idx: token for idx, token in enumerate(tokens)}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encodes text using the trained BPE model.\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to encode\n",
        "\n",
        "        Returns:\n",
        "            list: List of token IDs\n",
        "        \"\"\"\n",
        "        if not self.merges:\n",
        "            raise ValueError(\"Model must be trained before encoding\")\n",
        "\n",
        "        words = self._preprocess(text)\n",
        "        encoded_ids = []\n",
        "\n",
        "        for word in words:\n",
        "            # Initialize word as character sequence with end token\n",
        "            word_tokens = list(word) + ['</w>']\n",
        "\n",
        "            # Apply merges in order of priority\n",
        "            while len(word_tokens) > 1:\n",
        "                pairs = [(word_tokens[i], word_tokens[i+1])\n",
        "                         for i in range(len(word_tokens)-1)]\n",
        "\n",
        "                # Find the highest-priority merge (lowest rank)\n",
        "                best_pair = None\n",
        "                best_rank = float('inf')\n",
        "                best_idx = -1\n",
        "\n",
        "                for i, pair in enumerate(pairs):\n",
        "                    if pair in self.merges and self.merges[pair] < best_rank:\n",
        "                        best_pair = pair\n",
        "                        best_rank = self.merges[pair]\n",
        "                        best_idx = i\n",
        "\n",
        "                # If no merge is applicable, we're done\n",
        "                if best_pair is None:\n",
        "                    break\n",
        "\n",
        "                # Apply the merge\n",
        "                word_tokens = (word_tokens[:best_idx] +\n",
        "                               [''.join(best_pair)] +\n",
        "                               word_tokens[best_idx+2:])\n",
        "\n",
        "            # Convert tokens to IDs, handling unknown tokens\n",
        "            for token in word_tokens:\n",
        "                if token in self.token_to_id:\n",
        "                    encoded_ids.append(self.token_to_id[token])\n",
        "                else:\n",
        "                    # Handle unknown tokens\n",
        "                    print(f\"Warning: Unknown token '{token}'\")\n",
        "\n",
        "        return encoded_ids\n",
        "\n",
        "    def decode(self, encoded_ids):\n",
        "        \"\"\"Decodes the encoded IDs back into text.\n",
        "\n",
        "        Args:\n",
        "            encoded_ids (list): List of token IDs\n",
        "\n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        if not self.id_to_token:\n",
        "            raise ValueError(\"Model must be trained before decoding\")\n",
        "\n",
        "        decoded_text = []\n",
        "        current_word = []\n",
        "\n",
        "        for token_id in encoded_ids:\n",
        "            if token_id not in self.id_to_token:\n",
        "                print(f\"Warning: Unknown token ID {token_id}\")\n",
        "                continue\n",
        "\n",
        "            token = self.id_to_token[token_id]\n",
        "\n",
        "            # If this is the end-of-word token\n",
        "            if token == '</w>':\n",
        "                # Join and add the current word\n",
        "                decoded_text.append(''.join(current_word))\n",
        "                current_word = []\n",
        "            # If this token contains the end-of-word marker\n",
        "            elif '</w>' in token:\n",
        "                # Split the token and add the word part\n",
        "                word_part = token.replace('</w>', '')\n",
        "                current_word.append(word_part)\n",
        "                # Join and add the current word\n",
        "                decoded_text.append(''.join(current_word))\n",
        "                current_word = []\n",
        "            else:\n",
        "                # Add token to current word\n",
        "                current_word.append(token)\n",
        "\n",
        "        # Handle any remaining tokens in the buffer\n",
        "        if current_word:\n",
        "            decoded_text.append(''.join(current_word))\n",
        "\n",
        "        return ' '.join(decoded_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "4BfIFjxyTidl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Training BPE on a small corpus"
      ],
      "metadata": {
        "id": "zyIAiMFxHIMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm using a small corpus for demonstration.\n",
        "\n",
        "`vocab_size` is hyper-param representing the maximum number of unique subwords that BPE will learn and include in its vocabulary"
      ],
      "metadata": {
        "id": "fOGfRuFgoWcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"The quick brown fox jumps over the lazy dog.\n",
        "A quicker brown fox leaps over the lazy dog.\n",
        "The lazy dog barks at the quick brown fox.\n",
        "The fox, quick and brown, jumps over the lazy dog.\n",
        "The dog, lazy and sleepy, ignores the quick brown fox.\n",
        "A quick brown fox and a lazy dog live in harmony.\n",
        "The quick brown fox is faster than the lazy dog.\n",
        "The lazy dog is slower than the quick brown fox.\n",
        "The fox and the dog are friends, despite their differences.\n",
        "Quick brown foxes are rare, but lazy dogs are common.\n",
        "The quick brown fox is a symbol of agility.\n",
        "The lazy dog is a symbol of relaxation.\n",
        "Quick and brown, the fox is always on the move.\n",
        "Lazy and sleepy, the dog is always at rest.\n",
        "The quick brown fox and the lazy dog are opposites.\n",
        "Yet, they coexist in the same environment.\n",
        "The fox jumps, the dog barks, and life goes on.\"\"\"\n",
        "\n",
        "# Create and train BPE model\n",
        "bpe_model = BPE(vocab_size=50)\n",
        "bpe_model.train(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z17HxrEPmREa",
        "outputId": "52c2dd77-407f-4d4b-cd44-f467e9347701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#----------Training Started-----------#\n",
            "Iteration 0: Merging ('e', '</w>') with frequency 33\n",
            "Iteration 1: Merging ('t', 'h') with frequency 29\n",
            "Iteration 2: Merging ('th', 'e</w>') with frequency 25\n",
            "Iteration 3: Merging ('.', '</w>') with frequency 17\n",
            "Iteration 4: Merging ('s', '</w>') with frequency 16\n",
            "Iteration 5: Merging ('n', '</w>') with frequency 15\n",
            "Iteration 6: Merging ('f', 'o') with frequency 14\n",
            "Iteration 7: Merging ('fo', 'x') with frequency 14\n",
            "Iteration 8: Merging ('d', 'o') with frequency 14\n",
            "Iteration 9: Merging ('do', 'g') with frequency 14\n",
            "Iteration 10: Merging ('r', 'o') with frequency 13\n",
            "Iteration 11: Merging ('l', 'a') with frequency 13\n",
            "Iteration 12: Merging ('y', '</w>') with frequency 13\n",
            "Iteration 13: Merging ('q', 'u') with frequency 12\n",
            "Iteration 14: Merging ('qu', 'i') with frequency 12\n",
            "Iteration 15: Merging ('qui', 'c') with frequency 12\n",
            "Iteration 16: Merging ('quic', 'k') with frequency 12\n",
            "Iteration 17: Merging ('b', 'ro') with frequency 12\n",
            "Iteration 18: Merging ('bro', 'w') with frequency 12\n",
            "Iteration 19: Merging ('la', 'z') with frequency 12\n",
            "Iteration 20: Merging ('laz', 'y</w>') with frequency 12\n",
            "Iteration 21: Merging ('quick', '</w>') with frequency 11\n",
            "Iteration 22: Merging (',', '</w>') with frequency 11\n",
            "Iteration 23: Merging ('brow', 'n</w>') with frequency 10\n",
            "Iteration 24: Merging ('fox', '</w>') with frequency 9\n",
            "Iteration 25: Merging ('n', 'd') with frequency 9\n",
            "Iteration 26: Merging ('dog', '</w>') with frequency 8\n",
            "Iteration 27: Merging ('a', 'r') with frequency 8\n",
            "Iteration 28: Merging ('a', 'nd') with frequency 8\n",
            "Iteration 29: Merging ('and', '</w>') with frequency 8\n",
            "Iteration 30: Merging ('e', 'r') with frequency 7\n",
            "Iteration 31: Merging ('er', '</w>') with frequency 6\n",
            "Iteration 32: Merging ('i', 's</w>') with frequency 6\n",
            "Iteration 33: Merging ('a', '</w>') with frequency 5\n",
            "Iteration 34: Merging ('o', 'v') with frequency 4\n",
            "Iteration 35: Merging ('dog', '.</w>') with frequency 4\n",
            "Iteration 36: Merging ('t', '</w>') with frequency 4\n",
            "Iteration 37: Merging ('o', 'n') with frequency 4\n",
            "Iteration 38: Merging ('ar', 'e</w>') with frequency 4\n",
            "Iteration 39: Merging ('e', 's') with frequency 4\n",
            "Iteration 40: Merging ('j', 'u') with frequency 3\n",
            "Iteration 41: Merging ('ju', 'm') with frequency 3\n",
            "Iteration 42: Merging ('jum', 'p') with frequency 3\n",
            "Iteration 43: Merging ('ov', 'er</w>') with frequency 3\n",
            "Iteration 44: Merging ('l', 'e') with frequency 3\n",
            "Iteration 45: Merging ('fox', '.</w>') with frequency 3\n",
            "Iteration 46: Merging ('e', 's</w>') with frequency 3\n",
            "Iteration 47: Merging ('l', 'i') with frequency 3\n",
            "Iteration 48: Merging ('s', ',</w>') with frequency 3\n",
            "Iteration 49: Merging ('e', 'n') with frequency 3\n",
            "#----------Training Finished with 50 merges-----------#\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BPE at 0x7b110454d4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print learned vocab"
      ],
      "metadata": {
        "id": "YtvW3nhCKPDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "print(sorted(list(bpe_model.token_to_id.keys()),key=lambda x: len(x), reverse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9U6OA6IGplA",
        "outputId": "db8c077c-4ade-4069-9d65-a67ec4582933",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown</w>', 'quick</w>', 'dog.</w>', 'fox.</w>', 'lazy</w>', 'over</w>', 'and</w>', 'are</w>', 'dog</w>', 'fox</w>', 'the</w>', 'er</w>', 'es</w>', 'is</w>', 's,</w>', ',</w>', '.</w>', 'a</w>', 'e</w>', 'n</w>', 'quick', 's</w>', 't</w>', 'y</w>', '</w>', 'brow', 'jump', 'dog', 'fox', 'ar', 'en', 'er', 'es', 'la', 'le', 'li', 'nd', 'on', 'ov', 'ro', 'th', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Test the trained BPE"
      ],
      "metadata": {
        "id": "5ityGrDLOR7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test encoding and decoding\n",
        "test_text = \"the quickest brownest fox\"\n",
        "encoded = bpe_model.encode(test_text)\n",
        "decoded = bpe_model.decode(encoded)\n",
        "\n",
        "print(\"\\nInput text:\", test_text)\n",
        "print(f\"\\nTokenized text: {[bpe_model.id_to_token[token_id] for token_id in encoded]}\")\n",
        "print(\"\\nToken IDs:\", encoded)\n",
        "print(\"\\nDecoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K__bM3EsF28",
        "outputId": "63610f0a-a607-4cc4-c3b4-c8e22c72f603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input text: the quickest brownest fox\n",
            "\n",
            "Tokenized text: ['the</w>', 'quick', 'es', 't</w>', 'brow', 'n', 'es', 't</w>', 'fox</w>']\n",
            "\n",
            "Token IDs: [57, 47, 21, 55, 9, 39, 21, 55, 26]\n",
            "\n",
            "Decoded: the quickest brownest fox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "fHKR6fguSiAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input text legth: \",len(list(test_text)))\n",
        "print(\"Tokenized text length: \",len(list(encoded)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bPXm0Qrrz2g",
        "outputId": "8e62fad8-3c82-4a6a-997b-d45d737086cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text legth:  25\n",
            "Tokenized text length:  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 25-character sentence `\"the quickest brownest fox\"` was encoded into just 9 token IDs using BPE. If we didn't use BPE, we would have needed a token per character, resulting in 25 tokens, which demonstrates a reduction of about two-thirds in input length"
      ],
      "metadata": {
        "id": "Su9vKV1-1xju"
      }
    }
  ]
}