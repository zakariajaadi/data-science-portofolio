{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyP0oZgmu2Y57jbHcGE/ru95"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11079937,"sourceType":"datasetVersion","datasetId":6905773}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building a Financial Analyst Chatbot with RAG ğŸ¤–ğŸ“ˆ\n\nTraditional financial analysis is a laborious process - sifting through densely packed reports, extracting the relevant numbers, connecting disparate data points, and interpreting what it all means. But what if you could simply have a conversation about finances and get instant, accurate insights?\n\nThat's what I've built here by combining the reasoning capabilities of a LLM model (Mistral Instruct v3) with the factual precision of Retrieval-Augmented Generation (RAG) using LVMH financial reports as the knowledge source for this demonstration. \n\nWith RAG, the chatbot doesn't just respond with generic information - it dives into actual financial documents, extracts the most relevant data, and delivers insights through natural conversation.\n\nThis Notebook Covers:\n\n* Building a RAG Pipeline using `llama-index`\n* Building a text generation pipleline for the chatbot using `Mistral-7B-Instruct-v0.3`\n* Building a chatbot interface using `Gradio UI`\n\n\nLet's dive in! ğŸš€","metadata":{}},{"cell_type":"code","source":"!pip install -q -U llama-index\n!pip install -q -U llama-index-embeddings-huggingface\n!pip install -q -U optimum\n!pip install -q -U bitsandbytes\n!pip install -q -U gradio","metadata":{"id":"h8LU8bStWqbs","executionInfo":{"status":"ok","timestamp":1742333682862,"user_tz":-60,"elapsed":18763,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:16:27.477060Z","iopub.execute_input":"2025-03-19T15:16:27.477279Z","iopub.status.idle":"2025-03-19T15:17:05.877314Z","shell.execute_reply.started":"2025-03-19T15:16:27.477258Z","shell.execute_reply":"2025-03-19T15:17:05.876436Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m264.5/264.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport gradio as gr\nfrom threading import Thread\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer\n\n\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n\n\nimport os\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:18:09.508545Z","iopub.execute_input":"2025-03-19T15:18:09.508917Z","iopub.status.idle":"2025-03-19T15:18:36.655448Z","shell.execute_reply.started":"2025-03-19T15:18:09.508888Z","shell.execute_reply":"2025-03-19T15:18:36.654718Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"HF_TOKEN\"]= UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:18:41.224482Z","iopub.execute_input":"2025-03-19T15:18:41.224798Z","iopub.status.idle":"2025-03-19T15:18:41.428388Z","shell.execute_reply.started":"2025-03-19T15:18:41.224774Z","shell.execute_reply":"2025-03-19T15:18:41.427765Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Load quantized mistral instruct model","metadata":{"id":"rd1-znHGBgVn"}},{"cell_type":"code","source":"# NF4 Quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:18:39.554188Z","iopub.execute_input":"2025-03-19T15:18:39.555414Z","iopub.status.idle":"2025-03-19T15:18:39.562387Z","shell.execute_reply.started":"2025-03-19T15:18:39.555378Z","shell.execute_reply":"2025-03-19T15:18:39.561386Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Model checkpoint\nmodel_checkpoint = \"mistralai/Mistral-7B-Instruct-v0.3\"\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n# Load Model\nmodel = AutoModelForCausalLM.from_pretrained(\n        model_checkpoint,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlPFzikTk9tP","executionInfo":{"status":"ok","timestamp":1742333698755,"user_tz":-60,"elapsed":12586,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"outputId":"900c46f2-5cac-4d5b-8412-144ac358efdf","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:18:42.746526Z","iopub.execute_input":"2025-03-19T15:18:42.746883Z","iopub.status.idle":"2025-03-19T15:20:47.487416Z","shell.execute_reply.started":"2025-03-19T15:18:42.746854Z","shell.execute_reply":"2025-03-19T15:20:47.486546Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2fc2f9119f845cabcd73b6696f1941a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a951cdb67248958f179589423f68f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581aa71778c442009a66b84fe686ffc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6c7bfecb5c44ef87d23af8d19b6340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"298ee6437adf47feb4a2a37d0293fb34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abee5acdabb14b34badb734a41012b5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed2430a7c084b1ea4f471cbc54d681d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96feccc2597e46089bb28fac9b766de5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80220569c23404abbf55e6d079be6c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83886fcbc3d74702ba1513698b4043d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de89d48469da4fb080967463d12f09c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375d9c304d014db98b02f0c5fabc15fa"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# RAG (Retrieval Augmented Generation)","metadata":{"id":"5pIEoc9MBb10"}},{"cell_type":"markdown","source":"Load an embedding model","metadata":{}},{"cell_type":"code","source":"embedding_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:58:27.157112Z","iopub.execute_input":"2025-03-19T16:58:27.157494Z","iopub.status.idle":"2025-03-19T16:58:28.651895Z","shell.execute_reply.started":"2025-03-19T16:58:27.157464Z","shell.execute_reply":"2025-03-19T16:58:28.651097Z"}},"outputs":[],"execution_count":121},{"cell_type":"markdown","source":"RAG code","metadata":{}},{"cell_type":"code","source":"class RAGSystem:\n    def __init__(self,\n                 dir_path,\n                 embedding_model,\n                 chunk_size=256,\n                 chunk_overlap=25,\n                 top_k=3,\n                 similarity_threshold=0.5):\n\n       # Configure global settings\n       Settings.embed_model = embedding_model\n       Settings.llm = None  # Focus only on embedding generation\n       Settings.chunk_size = chunk_size\n       Settings.chunk_overlap = chunk_overlap\n\n       # Attributes\n       self.dir_path = dir_path\n       self.embedding_model = embedding_model\n       self.top_k = top_k\n       self.similarity_threshold = similarity_threshold\n       self.documents = self._load_documents()\n       self.index = self._create_index()\n       self.query_engine = self._configure_query_engine()\n\n    def _load_documents(self):\n        \"\"\"Load documents from the specified path.\"\"\"\n        reader = SimpleDirectoryReader(self.dir_path)\n        return reader.load_data()\n\n    def _create_index(self):\n        \"\"\"Create vector index from documents.\"\"\"\n        # High level transformation API : accepts an array of Document objects to parse and chunk them up\n        return VectorStoreIndex.from_documents(self.documents)\n\n    def _configure_query_engine(self):\n        \"\"\"Configure the retrieval query engine.\"\"\"\n        retriever = VectorIndexRetriever(\n            index=self.index,\n            similarity_top_k=self.top_k\n        )\n\n        return RetrieverQueryEngine(\n            retriever=retriever,\n            node_postprocessors=[\n                SimilarityPostprocessor(similarity_cutoff=self.similarity_threshold)\n            ]\n        )\n\n    def build_prompt(self, query):\n        \"\"\"Build a RAG prompt with retrieved context.\"\"\"\n\n        # retrieve knowledge\n        response = self.query_engine.query(query)\n        context_parts = []\n        for node in response.source_nodes:\n\n            # Extract node source\n            file_path = node.metadata.get(\"file_path\", \"Unknown File\")\n            file_name = os.path.basename(file_path)  \n            page_number = node.metadata.get(\"page_label\", \"Unknown Page\") \n            source=f\"{file_name}:{page_number}\"\n            source_info = f\"Source : [file: {file_name} , page: {page_number}]\"\n\n            # Node text\n            node_text=node.text\n            \n            # Add node text and source info to context\n            context_parts.append(f\"{source_info}\\n{node_text}\\n\")\n\n\n        context = \"\\n --- \\n\".join(context_parts)\n\n        return self._prompt_template(context, query)\n\n    def _prompt_template(self, context, query):\n\n        \"\"\"Format the final prompt with context and query.\"\"\"\n        prompt_template=f\"\"\"\n                        Context information is below.\n                        ---------------------                    \n                        \n                        {context}\n                    \n                        ---------------------\n                        Given the context information and not prior knowledge, answer the following query.\n                        Query: {query}\n                        \"\"\"\n    \n        return prompt_template\n\n    #def generate_response(self, query, llm):\n        #\"\"\"Generate a response using the RAG system and an LLM.\"\"\"\n        #prompt = self.build_prompt(query)\n        #return llm.generate(prompt)","metadata":{"id":"lp56oJ1nBZoJ","executionInfo":{"status":"ok","timestamp":1742333706219,"user_tz":-60,"elapsed":1333,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:58:30.506067Z","iopub.execute_input":"2025-03-19T16:58:30.506351Z","iopub.status.idle":"2025-03-19T16:58:30.515198Z","shell.execute_reply.started":"2025-03-19T16:58:30.506325Z","shell.execute_reply":"2025-03-19T16:58:30.514218Z"},"_kg_hide-input":true},"outputs":[],"execution_count":122},{"cell_type":"markdown","source":"# Text Generation code for the chatbot","metadata":{"id":"O7Cr_U7WDS02"}},{"cell_type":"code","source":"def generate_resp(chat, tokenizer, model, temperature):\n    \"\"\"\n        Generates model response using chat history.\n    \"\"\"\n    # Ensure inference mode\n    model.eval()\n\n    # Apply the chat template\n    formatted_chat = tokenizer.apply_chat_template(chat,\n                                                  tokenize=False,\n                                                  add_generation_prompt=True\n                                                  )\n\n    # Tokenize the chat\n    inputs = tokenizer(formatted_chat,\n                      return_tensors=\"pt\",\n                      add_special_tokens=False)\n\n    # Move the tokenized inputs and attention masks to the same device the model is on\n    inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n\n    # Initialize streamer to handle tokens as they are generated (we pss tokenizer for automatic decoding)\n    streamer = TextIteratorStreamer(tokenizer,\n                                    skip_special_tokens=True,\n                                    skip_prompt=True)\n\n    # Set generation parameters\n    generation_kwargs = dict(\n        **inputs,\n        streamer=streamer,\n        max_new_tokens=512,\n        do_sample=True,\n        temperature=temperature,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    # Run generation in a separate thread\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n\n    return streamer","metadata":{"id":"1dbRy4qfDSBg","executionInfo":{"status":"ok","timestamp":1742334696212,"user_tz":-60,"elapsed":18,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:58:34.729031Z","iopub.execute_input":"2025-03-19T16:58:34.729305Z","iopub.status.idle":"2025-03-19T16:58:34.735092Z","shell.execute_reply.started":"2025-03-19T16:58:34.729283Z","shell.execute_reply":"2025-03-19T16:58:34.734190Z"}},"outputs":[],"execution_count":123},{"cell_type":"markdown","source":"# Chatbot function and UI","metadata":{}},{"cell_type":"markdown","source":"Initialize RAG","metadata":{}},{"cell_type":"code","source":"rag_system = RAGSystem(\n        top_k=3,\n        similarity_threshold=0.5,\n        dir_path=\"/kaggle/input/lvmh-financial-report-pdf\",\n        embedding_model=embedding_model,\n        chunk_size=500,\n        chunk_overlap=50\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGj7X5jDIGFU","executionInfo":{"status":"ok","timestamp":1742334706271,"user_tz":-60,"elapsed":6391,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"outputId":"0e8de37a-8445-4ea6-915f-2f4236f6b102","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:58:37.882155Z","iopub.execute_input":"2025-03-19T16:58:37.882427Z","iopub.status.idle":"2025-03-19T16:58:46.144973Z","shell.execute_reply.started":"2025-03-19T16:58:37.882402Z","shell.execute_reply":"2025-03-19T16:58:46.144132Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"LLM is explicitly disabled. Using MockLLM.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd4d5b850cf477080af044a015debc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09f19e23ee9f4a1c9b95a4a93c057c9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48723aeaa5434f7c99e9477f368ee335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca635827114452f935e45157909897f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa43fc9e569d471eb625d0f6a8254e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd994964df64fcf97b2e81f1e3a470b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e31cab7b6184820950dc40b072b48e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f103416ba9e4bf587e0d79dbc40c08b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"735b03b3588049dfb797cbfc51fcb805"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08614cf49ba74d44a57350ccc6ecbe99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d37f566ef064dfdae72c0c875b394c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52ae66e90eb445aa1c13386b86ea2c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18b91f3388044978938227bc6cd6cc13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3361d9a380ea4fedbc4ffb37a20a188c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a29a0f0e3dcd42cfbf9bcc37ef40293c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6b3de540e043b2978c2e46f8b9aeed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f538e5abe9164c08a91516f0f1bda3ae"}},"metadata":{}}],"execution_count":125},{"cell_type":"markdown","source":"Define chatbot function","metadata":{}},{"cell_type":"code","source":"system_prompt= f\"\"\"\nYou are a financial analyst specializing in corporate earnings reports. \nYour task is to analyze the LVMH financial reports and provide accurate, concise, and well-structured responses.\n\n- Present financial data in a clear, structured manner, using bullet points or tables when necessary.\n- Where applicable, compare figures with previous years to highlight trends.\n- Always cite the source at the end of your answer, following this format \"Source : (file name, page number)\"\n\nMaintain a professional and neutral tone, avoiding unnecessary elaboration. \nYour goal is to provide **precise, data-driven insights** for financial analysis.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:35:23.836505Z","iopub.execute_input":"2025-03-19T17:35:23.836955Z","iopub.status.idle":"2025-03-19T17:35:23.840551Z","shell.execute_reply.started":"2025-03-19T17:35:23.836927Z","shell.execute_reply":"2025-03-19T17:35:23.839707Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"def chat_interface(message, history):\n    \"\"\" Gradio function.\"\"\"\n    # Initialize history with system prompt\n    if not history:\n        history.append({\"role\": \"system\", \"content\": system_prompt})\n   \n    # Get RAG prompt\n    prompt = rag_system.build_prompt(message)\n\n    # Prepare chat concatenating history and user input\n    chat = history + [{\"role\": \"user\", \"content\": prompt}]\n\n    # Get the streamer object that will yield generated text\n    streamer = generate_resp(chat, tokenizer, model, temperature=0.1)\n\n    # Streaming response\n    response = \"\"\n    for new_text in streamer:\n        response += new_text\n        yield response","metadata":{"id":"obEs4oD-D8N6","executionInfo":{"status":"ok","timestamp":1742334707487,"user_tz":-60,"elapsed":210,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:35:38.825997Z","iopub.execute_input":"2025-03-19T17:35:38.826276Z","iopub.status.idle":"2025-03-19T17:35:39.027503Z","shell.execute_reply.started":"2025-03-19T17:35:38.826255Z","shell.execute_reply":"2025-03-19T17:35:39.026666Z"}},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":"Define chatbot interface","metadata":{}},{"cell_type":"code","source":"chatbot=gr.ChatInterface(fn=chat_interface,\n                 type=\"messages\",\n                 examples=[\"What are the key financial highlights of 2024?\",\n                           \"What was the revenue distribution by geographic region in 2024?\",\n                           \"How efficient is LVMH in managing its assets (ROA for 2024)\",\n                           \"is LVMH positioned for growth in 2025?\",\n                           \"What are the key financial risks LVMH might face in the coming years?\",\n                           \"Does LVMH's financial data suggest that it's more dependent on organic growth or acquisitions?\"\n                           ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Launch the chatbot","metadata":{}},{"cell_type":"code","source":"chatbot.launch()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UMdWZAcnGprh","executionInfo":{"status":"ok","timestamp":1742334748576,"user_tz":-60,"elapsed":39170,"user":{"displayName":"Zakaria Jaadi","userId":"01123758218657716893"}},"outputId":"0c49e33f-fc0d-4bf4-eaa6-ee652459de55","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:35:39.336166Z","iopub.execute_input":"2025-03-19T17:35:39.336462Z","iopub.status.idle":"2025-03-19T17:35:39.871974Z","shell.execute_reply.started":"2025-03-19T17:35:39.336439Z","shell.execute_reply":"2025-03-19T17:35:39.871181Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7893\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://c072d02eb194e955c8.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://c072d02eb194e955c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":143,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644427cd9a784407800d6add04ff0f75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4118a48f6dd4bcda1d8d07d5a54fc4c"}},"metadata":{}}],"execution_count":143}]}